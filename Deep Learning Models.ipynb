{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Dropout, Input, LSTM, Bidirectional,GRU\n",
    "from keras.layers import MaxPooling1D, Conv1D, Flatten\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import (\n",
    "    classification_report as creport\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = pd.read_csv('dev_data.csv')\n",
    "dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be783b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data shape: {} \\nDev data shape: {}\".format(train_data.shape,dev_data.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(word_index, embedding_index, vocab_dim):\n",
    "    print('Building embedding matrix...')\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, vocab_dim))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index.get_vector(word)\n",
    "        except:\n",
    "            pass\n",
    "    print('Embedding matrix built.') \n",
    "    #print(\"Word index\", word_index.items())\n",
    "    #print(embedding_matrix) \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def get_init_parameters(path, ext=None):\n",
    "    if ext == 'vec':\n",
    "        word_model = KeyedVectors.load_word2vec_format(path).wv\n",
    "    else:\n",
    "        word_model = KeyedVectors.load(path).wv\n",
    "    n_words = len(word_model.vocab)\n",
    "    vocab_dim = word_model[word_model.index2word[0]].shape[0]\n",
    "    index_dict = dict()\n",
    "    for i in range(n_words):\n",
    "        index_dict[word_model.index2word[i]] = i+1\n",
    "    print('Number of words in the word embedding',n_words)\n",
    "    #print('word_model', word_model)\n",
    "    #print(\"index_dict\",index_dict)\n",
    "    return word_model, index_dict, n_words, vocab_dim\n",
    "\n",
    "def get_max_length(text_data, return_line=False):\n",
    "    max_length = 0\n",
    "    long_line = \"\"\n",
    "    for line in text_data:\n",
    "        new = len(line.split())\n",
    "        if new > max_length:\n",
    "            max_length = new\n",
    "            long_line = line\n",
    "    if return_line:\n",
    "        return long_line, max_length\n",
    "    else:\n",
    "        return max_length\n",
    "    print(\"max\",long_line,max_length)\n",
    "\n",
    "def load_datasets(data_paths, header=True):\n",
    "    x = []\n",
    "    y = []\n",
    "    for data_path in data_paths:\n",
    "        with open(data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if header:\n",
    "                    header = False\n",
    "                else:\n",
    "                    temp = line.split(',')\n",
    "                    x.append(temp[0])\n",
    "                    y.append(temp[2].replace('\\n', ''))\n",
    "    max_length = get_max_length(x)\n",
    "    print('Max length:', max_length)\n",
    "    return x,y, max_length\n",
    "\n",
    "def get_train_test(train_raw_text, test_raw_text, n_words, max_length):\n",
    "    tokenizer = text.Tokenizer(num_words=n_words)\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    word_index = tokenizer.word_index\n",
    "   \n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
    "           word_index\n",
    "\n",
    "def class_str_2_ind(x_train, x_test, y_train, y_test, classes, n_words, max_length):\n",
    "    print('Converting data to trainable form...')\n",
    "    y_encoder = preprocessing.LabelEncoder()\n",
    "    y_encoder.fit(classes)\n",
    "    y_train = y_encoder.transform(y_train)\n",
    "    y_test = y_encoder.transform(y_test)\n",
    "    #print(y_train)\n",
    "    #print(y_test)\n",
    "    train_y_cat = np_utils.to_categorical(y_train, len(classes))\n",
    "    x_vec_train, x_vec_test, word_index = get_train_test(x_train, x_test, n_words, max_length)\n",
    "    print('Number of training examples: ' + str(len(x_vec_train)))\n",
    "    print('Number of dev examples: ' + str(len(x_vec_test)))\n",
    "    return x_vec_train, x_vec_test, y_train, y_test, train_y_cat, word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "train_data_path=[\"/content/drive/My Drive/OSACT4/train_data_cleaned.csv\"]\n",
    "x_train, y_train, MAX_TEXT_LENGTH = load_datasets(train_data_path)\n",
    "CLASSES_LIST = np.unique(y_train)\n",
    "print('Label categories: ' + str(CLASSES_LIST))\n",
    "#0= HS, 1= NOT_HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91282762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dev data\n",
    "dev_data_path=[\"/content/drive/My Drive/OSACT4/dev_data_cleaned.csv\"]\n",
    "x_dev, y_dev, MAX_TEXT_LENGTH = load_datasets(dev_data_path)\n",
    "CLASSES_LIST = np.unique(y_dev)\n",
    "print('Label categories: ' + str(CLASSES_LIST))\n",
    "#0= HS, 1= NOT_HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXT_LENGTH=84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_dev, y_train, y_dev, train_y_cat, word_index = class_str_2_ind(x_train, x_dev, \n",
    "                                                                            y_train, y_dev,\n",
    "                                                                            CLASSES_LIST, MAX_FEATURES,\n",
    "                                                                            MAX_TEXT_LENGTH)\n",
    "dev_cat_y = np_utils.to_categorical(y_dev, len(CLASSES_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a28314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokens number: \"+str(len(word_index)))\n",
    "# Sequence length\n",
    "print(\"Original sequence length: \"+str(MAX_TEXT_LENGTH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e131cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding_weights, word_index, vocab_dim, max_length,layer, dropout, optimizer, print_summary=True):\n",
    "    \"\"\"\n",
    "    Create Neural Network With an Embedding layer\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(max_length,))\n",
    "    model = Embedding(input_dim=len(word_index)+1,\n",
    "                      output_dim=vocab_dim,\n",
    "                      trainable=False,\n",
    "                      weights=[embedding_weights])(inp)\n",
    "    model = layer(model)\n",
    "    model = Dropout(dropout)(model)       \n",
    "    model = Flatten()(model)\n",
    "    model = Dense(2, activation='sigmoid')(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
    "                   layer, dropout,optimizer):\n",
    "   \n",
    "    tmp = get_embedding_matrix(word_index, WORD_MODEL, EMBED_SIZE)\n",
    "    model = get_model(tmp, word_index, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
    "                      layer, dropout, optimizer= optimizer ,print_summary=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
    "\n",
    "def train_fit_predict(model, x_train, x_test, y_train, y_test,class_weight, batch_size, epochs, TestCallback=TestCallback):\n",
    "   \n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs, verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        class_weight=class_weight,\n",
    "                        callbacks=[TestCallback((x_test, y_test))])\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8faf5f",
   "metadata": {},
   "source": [
    "#  RNN (LSTM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17437e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
    "                       layer= LSTM(units=16, return_sequences=True, return_state=False), dropout=0.5, \n",
    "                       optimizer= optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "history, model = train_fit_predict(model,\n",
    "                               x_train[:, :MAX_TEXT_LENGTH],\n",
    "                               x_dev[:, :MAX_TEXT_LENGTH],\n",
    "                               train_y_cat, dev_cat_y, class_weight=None,\n",
    "                               batch_size=500, epochs=15)\n",
    "time_start = time() - time_start\n",
    "\n",
    "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921df298",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27648349",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_dev[:, :MAX_TEXT_LENGTH], dev_cat_y, batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6dd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(x_dev[:, :MAX_TEXT_LENGTH]), axis=1)\n",
    "\n",
    "print(creport(np.argmax(dev_cat_y, axis=1), y_pred,target_names=['HS', 'NOT_HS'],digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1810ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.argmin(history.history['val_loss'])\n",
    "\n",
    "print(\"Optimal epoch : {}\".format(n))\n",
    "print(\"Accuracy on train : {} %\".format(np.round(history.history['acc'][n]*100, 2)))\n",
    "print(\"Accuracy on val : {} %\".format(np.round(history.history['val_acc'][n]*100, 2)))\n",
    "print(\"Loss on train : {}\".format(np.round(history.history['loss'][n]*100, 2)))\n",
    "print(\"Loss on Val : {}\".format(np.round(history.history['val_loss'][n]*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7d1da",
   "metadata": {},
   "source": [
    "# RNN (BLSTM) Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ad3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
    "                       layer= Bidirectional(LSTM(units=32, return_sequences=True, return_state=False)), \n",
    "                       dropout=0.2, optimizer=optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c99d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "history, model = train_fit_predict(model,\n",
    "                               x_train[:, :MAX_TEXT_LENGTH],\n",
    "                               x_dev[:, :MAX_TEXT_LENGTH],\n",
    "                               train_y_cat, dev_cat_y, class_weight=None,\n",
    "                               batch_size=500, epochs=10)\n",
    "time_start = time() - time_start\n",
    "\n",
    "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22025479",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_dev[:, :MAX_TEXT_LENGTH], dev_cat_y, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e10b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(x_dev[:, :MAX_TEXT_LENGTH]), axis=1)\n",
    "\n",
    "print(creport(np.argmax(dev_cat_y, axis=1), y_pred,target_names=['HS', 'NOT_HS'],digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b44fa",
   "metadata": {},
   "source": [
    "# GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
    "                       layer= GRU(units=16, return_sequences=True, return_state=False), \n",
    "                       dropout=0.5, optimizer=optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "history, model = train_fit_predict(model,\n",
    "                               x_train[:, :MAX_TEXT_LENGTH],\n",
    "                               x_dev[:, :MAX_TEXT_LENGTH],\n",
    "                               train_y_cat, dev_cat_y, class_weight=None,\n",
    "                               batch_size=500, epochs=15)\n",
    "time_start = time() - time_start\n",
    "\n",
    "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_dev[:, :MAX_TEXT_LENGTH], dev_cat_y, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(x_dev[:, :MAX_TEXT_LENGTH]), axis=1)\n",
    "\n",
    "print(creport(np.argmax(dev_cat_y, axis=1), y_pred,target_names=['HS', 'NOT_HS'],digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
